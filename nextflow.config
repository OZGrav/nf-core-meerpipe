/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    nf-core/meerpipe Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Default config options for all compute environments
----------------------------------------------------------------------------------------
*/

manifest {
    // The metadata of the pipeline
    name = 'MeerPipe'
    homePage = 'https://github.com/OZGrav/meerpipe'
    description = 'Pulsar timing data processing pipeline for MeerTime data.'
    mainScript = 'meerpipe.nf'
    version = '3.0.0'
    author = 'Nick Swainston, Andrew Cameron, Aditya Parthasarathy, Stefan Oslowski, Andrew Jameson, Ren√©e Spiewak, Daniel Reardon, Matthew Bailes'
    defaultBranch = 'main'
    doi = '10.5281/zenodo.7918680'
}
params.manifest = manifest





// Global default params, used in configs
params {
    params.input = null // Not used, but required for nf-core/configs

    // Observation selection options
    list_in  = null  // List of observations to process, given in standard format.
                     // These will be crossmatched against PSRDB before job submission.
                     // List format must be:\n* Column 1 - Pulsar name\n* Column 2 - UTC\n* Column 3 - Observation PID\nTrailing columns may be left out if needed, but at a minimum the pulsar name must be provided.
    list_out = false // Output the list of observations you are processing and their info.
    utcs     = ""    // Start UTC for PSRDB search - returns only observations after this UTC timestamp
    utce     = ""    // End UTC for PSRDB search - returns only observations before this UTC timestamp
    obs_pid  = ""    // Project ID for PSRDB search - return only observations matching this Project ID. If not provided, returns all observations.
    pulsar   = ""    // Pulsar name for PSRDB search - returns only observations with this pulsar name. If not provided, returns all pulsars.

    // Processing options
    use_edge_subints = false // Use first and last 8 second subints of observation archives
    use_max_nsub = false // Use the maximum number of subints possible for the observation based on the S/N ratio
    tos_sn = 12 // Desired TOA S/N ratio, used to calculate the nsub to use
    nchans = "1" // List of nchans to frequency scrunch the data into
    npols = "1,4" // List of number of polarisations to scrunch the data into


    // MeerTime database options
    upload = true  // Upload result to the database
    psrdb_url   = "$PSRDB_URL" // URL for interacting with the database API
    psrdb_token = "$PSRDB_TOKEN" // Token taken from enviroment variable and obtained using get_ingest_token.sh or get_token.sh


    // Directory options (input/output)
    input_dir = "/fred/oz005/timing" // Path to the directories for each pulsar
    outdir    = "/fred/oz005/users/nswainst/meerpipe_testing_outputs" //Path where the data products are stored

    // Options taken from the Configuration file for MeerTime pipeline (meerpipe)
    type = "meertime" //Type of data

    // Base directory of ephemerides and template paths. There will be directorys for each project (and band for profiles) within this base directory
    ephemerides_dir = "/fred/oz005/users/meerpipe/ephemerides"
    templates_dir   = "/fred/oz005/users/meerpipe/templates"
    // Ephermiris and template locations. This can be overridden by a user to use none default files
    ephemeris = null
    template  = null



    // Boilerplate options
    tracedir                   = "${params.outdir}/pipeline_info"
    publish_dir_mode           = 'copy'
    email                      = null
    email_on_fail              = null
    plaintext_email            = false
    monochrome_logs            = false
    hook_url                   = null
    help                       = false
    version                    = false
    validate_params            = true
    show_hidden_params         = false
    schema_ignore_params       = 'genomes,validationSchemaIgnoreParams,manifest,validation-schema-ignore-params'
    validationShowHiddenParams = false
    validationSchemaIgnoreParams = false


    // Config options
    custom_config_version      = 'master'
    custom_config_base         = "https://raw.githubusercontent.com/nf-core/configs/${params.custom_config_version}"
    config_profile_description = null
    config_profile_contact     = null
    config_profile_url         = null
    config_profile_name        = null


    // Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                 = '128.GB'
    max_cpus                   = 16
    max_time                   = '240.h'

}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load nf-core custom profiles from different Institutions
try {
    includeConfig "${params.custom_config_base}/nfcore_custom.config"
} catch (Exception e) {
    System.err.println("WARNING: Could not load nf-core/config profiles: ${params.custom_config_base}/nfcore_custom.config")
}

// Load nf-core/meerpipe custom profiles from different institutions.
// Warning: Uncomment only if a pipeline-specific instititutional config already exists on nf-core/configs!
// try {
//   includeConfig "${params.custom_config_base}/pipeline/meerpipe.config"
// } catch (Exception e) {
//   System.err.println("WARNING: Could not load nf-core/config/meerpipe profiles: ${params.custom_config_base}/pipeline/meerpipe.config")
// }

def mem_calc(dur, task_attempt, ram_max) {
    // Return memory required by the job MB
    // dur is in seconds
    // task_attempt is the attempt number of the task
    // ram_max is macimum ram available for th job in GB
    ram_min       = 0.6 // GB
    ram_max       = ram_max.split("\\.")[0].toFloat()
    ram_slope     = 15.0
    ram_intercept = 0.6 // GB

    reqram = task_attempt * Float.valueOf(dur) * ram_slope + ram_intercept

    if ( reqram < ram_min ) {
        reqram = ram_min
    }
    else if ( reqram > ram_max ) {
        reqram = ram_max
    }
    return (int) ( reqram )
}


// CLUSTER SPECFIC DEFAULTS
// ----------------------------------------------------------------------------
def hostname = "hostname".execute().text.trim().replace("-", "")
params.hostname = hostname
if ( hostname.startsWith("tooarrana") || hostname.startsWith("trevor") ) {
    // Set up for OzStars new Ngarrgu Tindebeek cluster

    // Set up containers
    // process.module = 'apptainer'
    // singularity {
    //     enabled = true
    //     runOptions = '--nv -B /nvmetmp'
    //     envWhitelist = 'SINGULARITY_BINDPATH, SINGULARITYENV_LD_LIBRARY_PATH'
    // }
    // params.containerDir = '/pawsey/mwa/singularity'

    // Max resources for the milan partition
    params {
        max_memory                 = '256.GB'
        max_cpus                   = 62
        max_time                   = '7.day'
    }

    // Default directories
    workDir = "/fred/oz005/users/${USER}/work"

    process {
        cache = 'lenient'
        // Resource set up
        withLabel: cpu {
            // queue = 'workq'
            executor = 'slurm'
            errorStrategy = 'retry'
            maxRetries = 2
            queue  = 'milan'
            cpus   = 1
            time   = { "${task.attempt * dur.toFloat()} s" }
            memory = { "${mem_calc(dur, task.attempt, params.max_memory)} GB"}
        }
        withLabel: scratch {
            scratch = '$JOBFS'
            clusterOptions = { "--tmp=${(task.attempt * dur.toFloat() * 16).toInteger()}MB" }
        }
        // Software dependency set up
        withLabel: psrdb {
            if ( hostname.startsWith("tooarrana") ) {
                beforeScript = "module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles"//; module load psrdb/677938d"
            }
            else if ( hostname.startsWith("trevor") ) {
                beforeScript = "export SYS_ARCH=openstack; module use /apps/users/pulsar/openstack/gcc-11.3.0/modulefiles; module load psrdb/646a057"
            }
        }
        withLabel: meerpipe {
            beforeScript = 'source /fred/oz005/users/nswainst/code/meerpipe/env_setup.sh; source /home/nswainst/venv/bin/activate'
        }
        withLabel: psrchive {
            beforeScript = 'module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module load psrchive/c216582a0'
        }
        withLabel: coast_guard {
            beforeScript = 'module use /apps/users/pulsar/milan/gcc-11.3.0/modulefiles; module load coast_guard/56b8d81'
        }
    }
    executor.submitRateLimit = '100 sec'
    executor.$slurm.queueSize = 1000
}
else if ( hostname.startsWith("farnarkle") ) {
    // Set up for Swinburnes's Ozstar cluster
}
else {
    // No recognised hostname so assuming defaults

    // Resource set up
    executor.name = 'local'
    executor.queueSize = 8

    // Set up containers
    docker.enabled = true
}


profiles {
    debug {
        dumpHashes             = true
        process.beforeScript   = 'echo $HOSTNAME'
        cleanup = false
    }
    conda {
        conda.enabled          = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    mamba {
        conda.enabled          = true
        conda.useMamba         = true
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    docker {
        docker.enabled         = true
        docker.registry        = 'quay.io'
        docker.userEmulation   = true
        conda.enabled          = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    arm {
        docker.runOptions = '-u $(id -u):$(id -g) --platform=linux/amd64'
    }
    singularity {
        singularity.enabled    = true
        singularity.autoMounts = true
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    podman {
        podman.enabled         = true
        podman.registry        = 'quay.io'
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        shifter.enabled        = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    shifter {
        shifter.enabled        = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        charliecloud.enabled   = false
        apptainer.enabled      = false
    }
    charliecloud {
        charliecloud.enabled   = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        apptainer.enabled      = false
    }
    apptainer {
        apptainer.enabled      = true
        conda.enabled          = false
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    gitpod {
        executor.name          = 'local'
        executor.cpus          = 16
        executor.memory        = 60.GB
    }
    test      { includeConfig 'conf/test.config'      }
    test_full { includeConfig 'conf/test_full.config' }
}



// Export these variables to prevent local Python/R libraries from conflicting with those in the container
// The JULIA depot path has been adjusted to a fixed path `/usr/local/share/julia` that needs to be used for packages in the container.
// See https://apeltzer.github.io/post/03-julia-lang-nextflow/ for details on that. Once we have a common agreement on where to keep Julia packages, this is adjustable.

env {
    PYTHONNOUSERSITE = 1
    R_PROFILE_USER   = "/.Rprofile"
    R_ENVIRON_USER   = "/.Renviron"
    JULIA_DEPOT_PATH = "/usr/local/share/julia"
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.html"
}

manifest {
    name            = 'nf-core/meerpipe'
    author          = """Nick Swainston"""
    homePage        = 'https://github.com/nf-core/meerpipe'
    description     = """Pulsar timing data processing pipeline for MeerTime data."""
    mainScript      = 'main.nf'
    nextflowVersion = '!>=22.10.1'
    version         = '1.0dev'
    doi             = '10.5281/zenodo.7918680'
}
params.manifest = manifest

// Load modules.config for DSL2 module specific options
includeConfig 'conf/modules.config'

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
